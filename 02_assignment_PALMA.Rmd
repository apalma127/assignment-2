---
title: 'Assignment #2 - PALMA'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(naniar)            # for analyzing missing values
library(vip)               # for variable importance plots
theme_set(theme_minimal()) # Lisa's favorite theme
```

```{r data}
hotels <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv')
```

## Put it on GitHub!        

**Task**: When you are finished with the assignment, post a link below to the GitHub repo for the assignment. 

Link:  https://github.com/apalma127/assignment-2


**Tasks**:

1. Read about the hotel booking data, `hotels`, on the [Tidy Tuesday page](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md) it came from. There is also a link to an article from the original authors. The outcome we will be predicting is called `is_canceled`.

  - Without doing any analysis, what are some variables you think might be predictive and why?  
  
  **When predicting if a booking results in a cancellation, I would expect the following to be predictive:** 
            1. **deposit_type**: odds are if someone placed a deposit, they are more committed to the stay than no deposit; also, non refundable I would bet results in little to no cancels compared to refundable.
            2. **is_repeated_guest**:  if someone is a routine stayer, you would expect repeated guests to predict well verry little cancellations as they have reason to stay and also have in the past
            3. **lead_time**: if booked a lot out in advancr, you would think there would be less of a chance of cancellation compared to a late booking as far out in advance bookings most likely revolve around events that have been planned for a while
  
  _ What are some problems that might exist with the data? You might think about how it was collected and who did the collecting.  
  
  **One major problem is that this data is obtained and maintained by hotels who clearly have incentive to list to overstate how well they do.  By under reporting cancellations, the data may only reflect what the industry wants us to see: low amounts of cancellations, which reflect low amounts of lost revenue.  It is very quite possible heaps of cancellations are not included in this data to conceal some large sums of lost revenue that is being hidden.  I think this would be reporting bias.**
  
  
  - If we construct a model, what type of conclusions will be able to draw from it?  
  
**We can look at the sign, size, and significance of the coefficients of the variables and determine which factors result in directional trends in cancellations.  For example for is_repeated_guest, if our model finds this variable has a large negative coefficient, we can see that, relative to the reference 0 value not being a repeat customer, repeat customers are way less likely to cancel than non-repeat customers.**
  
  
2. **NEED TO FINISH**


Create some exploratory plots or table summaries of the variables in the dataset. Be sure to also examine missing values or other interesting values. You may want to adjust the `fig.width` and `fig.height` in the code chunk options.

```{r}
hotels %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(variable), 
             scales = "free")
```




**Missing Values and Points of Interest**

- is_cancelled: is pretty evenly spread as it relates to cancelled or not; it also has a lot of points being over 100k
- adr: looks funky with most rates seeming to be super similar; can't quite tell but not sure if there is enough distinction in rates just at first glance
- adults: tough to tell but looks like most had only a handful, not a big spread of data
- arrival_date_day_of_month: is a really weird variable but seems normally spread
- arrival_date_week_number: is weird as well but is pretty normally spread as well
- arrival_date_year: for some reason there are a lot of 


```{r}
hotels %>% 
  select(where(is.factor)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_bar() +
  facet_wrap(vars(variable), 
             scales = "free", 
             nrow = 2)
```


3. First, we will do a couple things to get the data ready. 

* I did the following for you: made outcome a factor (needs to be that way for logistic regression), made all character variables factors, removed the year variable and some reservation status variables, and removed cases with missing values (not NULLs but true missing values).

* You need to split the data into a training and test set, stratifying on the outcome variable, `is_canceled`. Since we have a lot of data, split the data 50/50 between training and test. I have already `set.seed()` for you. Be sure to use `hotels_mod` in the splitting.

```{r}
hotels_mod <- hotels %>% 
  mutate(is_canceled = as.factor(is_canceled)) %>% 
  mutate(across(where(is.character), as.factor)) %>% 
  select(-arrival_date_year,
         -reservation_status,
         -reservation_status_date) %>% 
  add_n_miss() %>% 
  filter(n_miss_all == 0) %>% 
  select(-n_miss_all)

set.seed(494)

hotels_split <- initial_split(hotels_mod, 
                             prop = .5)
hotels_split

hotels_training <- training(hotels_split)
hotels_testing <- testing(hotels_split)
```

4. In this next step, we are going to do the pre-processing. Usually, I won't tell you exactly what to do here, but for your first exercise, I'll tell you the steps. 

* Set up the recipe with `is_canceled` as the outcome and all other variables as predictors (HINT: `~.`).  
* Use a `step_XXX()` function or functions (I think there are other ways to do this, but I found `step_mutate_at()` easiest) to create some indicator variables for the following variables: `children`, `babies`, and `previous_cancellations`. So, the new variable should be a 1 if the original is more than 0 and 0 otherwise. Make sure you do this in a way that accounts for values that may be larger than any we see in the dataset.  
* For the `agent` and `company` variables, make new indicator variables that are 1 if they have a value of `NULL` and 0 otherwise. I also used `step_mutate_at()` for this, but there's more ways you could do it.
* Use `fct_lump_n()` inside `step_mutate()` to lump together countries that aren't in the top 5 most occurring. 
* If you used new names for some of the new variables you created, then remove any variables that are no longer needed. 
* Use `step_normalize()` to center and scale all the non-categorical predictor variables. (Do this BEFORE creating dummy variables. When I tried to do it after, I ran into an error - I'm still [investigating](https://community.rstudio.com/t/tidymodels-see-notes-error-but-only-with-step-xxx-functions-in-a-certain-order/115006) why.)
* Create dummy variables for all factors/categorical predictor variables (make sure you have `-all_outcomes()` in this part!!).  
* Use the `prep()` and `juice()` functions to apply the steps to the training data just to check that everything went as planned.

```{r}
hotels_recipe <- recipe(is_canceled ~ ., 
              data = hotels_training) %>% 
              step_mutate_at(children = as.numeric(children != 0),
                babies = as.numeric(babies != 0),
                previous_cancellations = as.numeric(previous_cancellations != 0),
                agent = as.numeric(agent == "NULL"), 
                company = as.numeric(company == "NULL")) %>%
              step_mutate(country = fct_lump_n(country, 5)) %>%
              step_normalize(all_predictors(), 
                         -all_nominal())%%
              step_dummy(all_nominal(),-all_outcomes())
              
```

```{r}
hotels_recipe %>% 
  prep(hotels_training) %>%
  juice() 
```


5. In this step we will set up a LASSO model and workflow.

* In general, why would we want to use LASSO instead of regular logistic regression? (HINT: think about what happens to the coefficients).  
**Lasso shrinks coefficients to 0 based on penalty terms.  It allows for more interpretable and meaningful coefficients for the more determinative variables.**

* Define the model type, set the engine, set the `penalty` argument to `tune()` as a placeholder, and set the mode. 
* Create a workflow with the recipe and model.  

```{r}
hotels_lasso_mod <- 
  linear_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("regression")
```

```{r}
hotels_lasso_wf <- 
  workflow() %>% 
  add_recipe(hotels_recipe) %>% 
  add_model(hotels_lasso_mod)

hotels_lasso_wf
```


6. In this step, we'll tune the model and fit the model using the best tuning parameter to the entire training dataset.

* Create a 5-fold cross-validation sample. We'll use this later. I have set the seed for you.  
* Use the `grid_regular()` function to create a grid of 10 potential penalty parameters (we're keeping this sort of small because the dataset is pretty large). Use that with the 5-fold cv data to tune the model.  
* Use the `tune_grid()` function to fit the models with different tuning parameters to the different cross-validation sets. 
* Use the `collect_metrics()` function to collect all the metrics from the previous step and create a plot with the accuracy on the y-axis and the penalty term on the x-axis. Put the x-axis on the log scale.  
* Use the `select_best()` function to find the best tuning parameter, fit the model using that tuning parameter to the entire training set (HINT: `finalize_workflow()` and `fit()`), and display the model results using `pull_workflow_fit()` and `tidy()`. 

```{r}
set.seed(494) # for reproducibility

hotels_cv <- vfold_cv(hotels_training, v = 5)

penalty_grid <- grid_regular(penalty(),
                             levels = 10)
penalty_grid 

```

```{r}
hotels_lasso_tune <- 
  hotels_lasso_wf %>% 
  tune_grid(
    resamples = hotels_cv,
    grid = penalty_grid
    )

hotels_lasso_tune
```


```{r}
hotels_lasso_tune %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  filter(.metric == "rmse")
```


```{r}
hotels_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") 
```

```{r}
hotels_lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10",scales::math_format(10^.x))) +
  labs(x = "penalty", y = "rmse")
```


```{r}
hotels_lasso_tune %>% 
  show_best(metric = "rmse")
```

```{r}
best_param <- hotels_lasso_tune %>% 
  select_best(metric = "rmse")
best_param
```


```{r}
hotels_lasso_final_wf <- hotels_lasso_wf %>% 
  finalize_workflow(best_param)
hotels_lasso_final_wf
```

**Are there some variables with coefficients of 0?**

**_____________________________________________________**
```{r}
hotels_lasso_final_mod <- hotels_lasso_final_wf %>% 
  fit(data = hotels_training)

hotels_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy() 
```


7. Now that we have a model, let's evaluate it a bit more. All we have looked at so far is the cross-validated accuracy from the previous step. 

* Create a variable importance graph. Which variables show up as the most important? Are you surprised?  
* Use the `last_fit()` function to fit the final model and then apply it to the testing data. Report the metrics from the testing data using the `collet_metrics()` function. How do they compare to the cross-validated metrics?

```{r}
hotels_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  vip()
```

Which variables show up as the most important? Are you surprised?

**____________________________________________**


```{r}
hotels_lasso_test <- hotels_lasso_final_wf %>% 
  last_fit(hotels_split)

hotels_lasso_test %>% 
  collect_metrics()
```


How do they compare to the cross-validated metrics?

**____________________________________________**


* Use the `collect_predictions()` function to find the predicted probabilities and classes for the test data. Save this to a new dataset called `preds`. Then, use the `conf_mat()` function from `dials` (part of `tidymodels`) to create a confusion matrix showing the predicted classes vs. the true classes. Compute the true positive rate (sensitivity), true negative rate (specificity), and accuracy. See this [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) reference if you (like me) tend to forget these definitions. Also keep in mind that a "positive" in this case is a cancellation (those are the 1's). 


```{r}

```


* Use the `preds` dataset you just created to create a density plot of the predicted probabilities of canceling (the variable is called `.pred_1`), filling by `is_canceled`. Use an `alpha = .5` and `color = NA` in the `geom_density()`. Answer these questions: 

```{r}

```


a. What would this graph look like for a model with an accuracy that was close to 1?  

**____**

b. Our predictions are classified as canceled if their predicted probability of canceling is greater than .5. If we wanted to have a high true positive rate, should we make the cutoff for predicted as canceled higher or lower than .5?  

**____**


c. What happens to the true negative rate if we try to get a higher true positive rate? 

**____**


8. Let's say that this model is going to be applied to bookings 14 days in advance of their arrival at each hotel, and someone who works for the hotel will make a phone call to the person who made the booking. During this phone call, they will try to assure that the person will be keeping their reservation or that they will be canceling in which case they can do that now and still have time to fill the room. How should the hotel go about deciding who to call? How could they measure whether it was worth the effort to do the calling? Can you think of another way they might use the model? 

**_____**

9. How might you go about questioning and evaluating the model in terms of fairness? Are there any questions you would like to ask of the people who collected the data? 


**____**

## Bias and Fairness

Matrix of Domination:

**It is always important to wonder about who is collecting data and why.  Everyone an d every entity has a motive usually with self interest in mind.  Groups actively overlooked and harmed are ones who are excluded from the field and from datatsets.  By not being in the field, there may be little to no advocacy for this group which can directly lead to exclusion from data and datasets, which are used for consequential decisions like redistricting and policiing, leading to a negative chain of events.**

Missing Data:

**As soon as I read missing data I immediately think of census returns, seeing articles describing how NY ended up losing a seat because they were 89 residents short.  If 89 more people in the massive state of NY felt like filling out the census was as consequential as it was, NY would have retained its entirety of the influence it holds in the electoral college.  This could be from a lack of feeling liek you matter and the government has your best interests in mind as well as a failure of education.**       

Interest/Goals: **It is extremely disheartening to hear about the many matrices and cycles of oppression throughout society.  One that has always baffled me has been the housing issue for people with criminal records.  If our system is meant to focus on rehab and reform (questionable), yet society is focused on punishing and pointing out, this inevitably leads to the cycle of recidivism, further burying people in a hopeless situation.**